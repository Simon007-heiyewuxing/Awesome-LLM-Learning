#### PPO (Proximal Policy Optimization) 介绍
[参考1](https://huggingface.co/blog/deep-rl-ppo), [参考2](https://zhuanlan.zhihu.com/p/645225982)

- 大语言模型 RLHF 中的 PPO 分成三部分：采样、反馈和学习
  - 采样就是学生回答问题的过程，是模型根据提示（prompt）输出回答（response）的过程，或者说是模型自行生产训练数据的过程
    - 策略（policy），它就是RLHF中的“学生”。policy由两个模型组成，一个叫做演员模型（Actor），另一个叫做评论家模型（Critic）。它们就像是学生大脑中的两种意识，一个负责决策，一个负责总结得失。其中演员就是我们想要训练出来的大模型。在用PPO训练它之前，它就是RLHF的第一步训练出来的SFT（Supervised Fine-Tuning）model。
  - 反馈就是老师检查答案的过程，是奖励模型（Reward Model）给response打分的过程，或者说是奖励模型给训练数据X标上Y值的过程
    - 给予最终奖励之前，最好也对它的“标新立异”给予少量的惩罚。实现方式是让 old_log_prob 的 token 概率分布尽量接近 ref_log_prob。通俗来说，整个reward function的计算逻辑是典型的霸总逻辑：除非你能拿到好的结果，否则你就得给我守规矩。
  - “学习“就是学生根据反馈总结得失并自我改进的过程，或者说是强化优势动作的过程。
    - 优势定义为“实际获得的收益超出预期的程度”。PPO计算优势的方法：优势 = 实际收益 - 预期收益。
    - 所谓“强化优势动作”，即强化那些展现出显著优势的动作。

#### DPO (Direct Preference Optimization) 介绍
[参考1](https://arxiv.org/pdf/2305.18290.pdf)，[参考2](https://zhuanlan.zhihu.com/p/636122434)
- 背景
  - 目前常用人类对模型生成数据的反馈来进一步训练LM，对齐人类偏好；RLHF是其中的代表工作，先通过训练一个反映人类对生成回答偏好的reward model（RW），再通过强化学习（PPO）来最大化预测的回报（reward），同时，施加KL限制（constraints）避免模型偏离太远。
  -这个 pipeline 比较复杂，需要先训练一个reward model 得到回报分数，再通过 PPO 强化学习最大化 reward 更新策略（模型参数），其中的 PPO 阶段，需要多个模型（actor&ref model& critic model& reward model），特别耗显存，并且在训练过程中，需要对策略进行采样，计算量巨大


- DPO 实现方式
  ![dpo pipeline](./images/dpo.png)
  - 直接优化 LM 来对齐人类偏好，无需建模 reward model 和强化学习阶段。基于 RL 的目标函数可以通过优化二分 cross entropy 目标来优化
  - DPO loss 表达为如下形式
  ![dpo loss](./images/dpo_1.png)
  其中 $y_w$ 的被人类喜好程度大于 $y_l$，DPO loss 的梯度如下
  ![dpo gradient](./images/dpo_2.png)
  可以看出该 loss 的作用主要是增加喜好数据的 likelihood，降低非喜好数据的 likelihood，同时会基于隐私 reward 估计的错误程度进行加权。本文的实验表明了这种加权的重要性，因为没有加权系数的这种方法的简单版本可能会导致语言模型退化
