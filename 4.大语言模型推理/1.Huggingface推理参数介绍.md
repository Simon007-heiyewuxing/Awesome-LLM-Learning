#### 温度系数（Temperature）对大模型推理结果的影响
参考：[What is Temperature in NLP / LLMs?](https://medium.com/@lazyprogrammerofficial/what-is-temperature-in-nlp-llms-aa2a7212e687), [NLP / LLMs中的Temperature 是什么?](https://blog.csdn.net/deephub/article/details/129682591)
- 温度系数直观解释
  - Temperature 是一个超参数，可用于控制生成语言模型中生成文本的随机性和创造性。它用于调整模型的softmax输出层中预测词的概率。温度参数定义为在应用 softmax 函数之前用于调整 logits 的比例因子的倒数。
  - 当Temperature 设置为较低的值时，预测词的概率会变尖锐，这意味着选择最有可能的词的概率更高。这会产生更保守和可预测的文本，因为模型不太可能生成意想不到或不寻常的词。另一方面，当Temperature 设置为较高值时，预测词的概率被拉平，这意味着所有词被选择的可能性更大。这会产生更有创意和多样化的文本，因为模型更有可能生成不寻常或意想不到的词。
  - 温度参数通常设置为 0.1 到 1.0 之间的值，具体取决于生成文本中所需的随机性和创造性水平。温度值为 1.0 对应于标准 softmax 函数，其中预测词的概率未按比例缩放。

- 温度系数数学解释
  - LLM 输出为每个单词的预测概率分布，由 softmax 函数计算
    $p(x)=\frac{e^{x_{i}}}{\sum_{N}^{j=1}{e^{x_{j}}}}$
  - 加上温度系数后的 softmax 函数形式为：
    $p(x)=\frac{e^{\frac{x_{i}}{T}}}{\sum_{N}^{j=1}{{e^{\frac{x_{j}}{T}}}}}$
    - 如果当T趋于无穷时会发生什么。每个x_i / T都会趋于0，从而得到一个均匀分布。也就是说概率分布变得更 “平”， 这会导致结果更随机。
    - 当T很小(比如0.1)时会发生什么。每个x_i / T之间的差异变得更加明显(例如5比1变成50比10)，这样概率分布变得“更尖”，也就是说结果会更确定。

#### Huggingface generate 函数中的 top_p sampling、top_k sampling、greedy_search、beam_search 参数解释
参考：[Huggingface 的 generate 方法介绍：top_p sampling、top_k sampling、greedy_search、beam_search](https://zhuanlan.zhihu.com/p/643949567)
- Greedy search 每次都选择概率最大的词作为单词序列中的下一个词
- beam search（束搜索）：通过在每个时间步骤保留最有可能的 num_beams 个假设，最终选择具有最高概率的假设，从而降低错过隐藏的高概率词序列的风险
- top-k sampling：在Top-K采样中，选择最有可能的 K 个下一个词，并将概率质量重新分配给这 K 个下一个词
- Top-p (nucleus) sampling：与仅从最可能的 K 个单词中进行采样不同，Top-p 采样从概率累积超过概率 p 的可能性最小的单词集中进行选择


#### Huggingface generate 生成文本重复有什么参数调节
[参考1](https://huggingface.co/blog/how-to-generate), [参考2](https://arxiv.org/pdf/1909.05858.pdf), [参考3](https://arxiv.org/abs/1705.04304)

基于以下参数能避免重复样本生成

- repetition_penalty
  - 文档：[LINK](https://huggingface.co/transformers/v4.1.1/internal/generation_utils.html#transformers.RepetitionPenaltyLogitsProcessor)
  - 源码：[LINK](https://huggingface.co/transformers/v4.1.1/_modules/transformers/generation_logits_process.html)
  - 参数作用：对重复序列实施惩罚，设置为 1.0 代表没有惩罚，一般设置为 1.1 或 1.2 就能缓解重复序列的生成
  - 实现原理：在语言模型输出的 logits 通过乘以设置的 `repetition_penalty` 来降低重复序列的权重，具体代码如下，对于小于 0 的部分乘以 penalty 参数，大于 0 的部分乘以 penalty 参数，这里 penalty 一般设置为 1~2 之间，所以能够降低重复序列的值。核心代码如下：
    ```python
    score = torch.where(score < 0, score * self.penalty, score / self.penalty)
    ```

- no_repeat_ngram_size
  - 文档：[LINK](https://huggingface.co/transformers/v4.1.1/internal/generation_utils.html#transformers.NoRepeatNGramLogitsProcessor)
  - 源码：[LINK](https://huggingface.co/transformers/v4.1.1/_modules/transformers/generation_logits_process.html)
  - 参数作用：
    - 对于包含了相同的单词序列的重复，设置该参数可以引入[论文](https://arxiv.org/abs/1705.04304)中介绍的 n-gram (即 n 个单词的单词序列) 对重复单词序列进行惩罚，确保不会出现重复的 n-gram 。设置 `no_repeat_ngram_size=n` 能够促使 n-gram 不会重复出现
  - 实现原理：基于输入的文本中统计已经出现的单词序列，将这部分单词序列的出现概率置 0。核心代码如下：
    ```python
    for i, banned_tokens in enumerate(banned_batch_tokens):
      scores[i, banned_tokens] = -float("inf")
    ```
    注意这里将 `logits` 置为负无穷后，经过 softmax 函数后就实现将概率置 0 了。